{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPiqTQ4jRhNcj7qsb29QUwb",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/VytautasBruzila/Baigiamasis-darbas-JackDaniels/blob/main/Baigiamasis_darbas_Jack_Daniels_check.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here’s a high-level overview of the steps you would need to follow:\n",
        "\n",
        "Image Preprocessing: You would need to preprocess your images before feeding them into the model. This could involve resizing the images to a fixed size, normalizing the pixel values, etc. Keras provides the ImageDataGenerator class that can help with this.\n",
        "Model Training: You would train your model on your preprocessed datasets. Make sure to divide your data into a training set and a validation set to evaluate your model’s performance.\n",
        "Prediction Function: After training your model, you can use it to predict the class of new images. You would need to preprocess these new images in the same way as your training and validation images.\n",
        "In this code, the predict_image function takes the path to an image file, preprocesses the image, and uses the trained model to predict whether the image is of an original or fake Jack Daniels bottle. You can call this function with the path to a new image to make a prediction.\n",
        "\n",
        "Please replace 'dataset/training_set' and 'dataset/test_set' with the paths to your actual datasets. The fit_generator function trains the model for a fixed number of epochs (iterations on a dataset). You can adjust the steps_per_epoch, epochs, and validation_steps parameters based on your dataset size and computational resources.\n",
        "\n",
        "Remember, this is a basic model and might not give high accuracy. For complex datasets, you might need a more complex model, data augmentation, or pre-trained models. Also, always remember to preprocess your images and labels correctly before feeding them into the model."
      ],
      "metadata": {
        "id": "GU7Tfwg6Tc0B"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sJ6Ze3mT-wED"
      },
      "outputs": [],
      "source": [
        "# Import necessary libraries\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras.preprocessing import image\n",
        "import numpy as np\n",
        "\n",
        "# Initialize the CNN\n",
        "model = Sequential()\n",
        "\n",
        "# Add the convolutional layer\n",
        "model.add(Conv2D(32, (3, 3), input_shape=(64, 64, 3), activation='relu'))\n",
        "\n",
        "# Add the pooling layer\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "\n",
        "# Add another convolutional layer\n",
        "model.add(Conv2D(32, (3, 3), activation='relu'))\n",
        "\n",
        "# Add another pooling layer\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "\n",
        "# Flattening\n",
        "model.add(Flatten())\n",
        "\n",
        "# Full connection\n",
        "model.add(Dense(units=128, activation='relu'))\n",
        "model.add(Dense(units=1, activation='sigmoid'))\n",
        "\n",
        "# Compile the CNN\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Part 2 - Fitting the CNN to the images\n",
        "train_datagen = ImageDataGenerator(rescale=1./255,\n",
        "                                   shear_range=0.2,\n",
        "                                   zoom_range=0.2,\n",
        "                                   horizontal_flip=True)\n",
        "\n",
        "test_datagen = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "training_set = train_datagen.flow_from_directory('dataset/training_set',\n",
        "                                                 target_size=(64, 64),\n",
        "                                                 batch_size=32,\n",
        "                                                 class_mode='binary')\n",
        "\n",
        "test_set = test_datagen.flow_from_directory('dataset/test_set',\n",
        "                                            target_size=(64, 64),\n",
        "                                            batch_size=32,\n",
        "                                            class_mode='binary')\n",
        "\n",
        "model.fit_generator(training_set,\n",
        "                    steps_per_epoch=8000,\n",
        "                    epochs=25,\n",
        "                    validation_data=test_set,\n",
        "                    validation_steps=2000)\n",
        "\n",
        "# Part 3 - Making new predictions\n",
        "def predict_image(file):\n",
        "    test_image = image.load_img(file, target_size = (64, 64))\n",
        "    test_image = image.img_to_array(test_image)\n",
        "    test_image = np.expand_dims(test_image, axis = 0)\n",
        "    result = model.predict(test_image)\n",
        "    if result[0][0] == 1:\n",
        "        prediction = 'fake'\n",
        "    else:\n",
        "        prediction = 'original'\n",
        "    return prediction\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "6BUB93vKwdBN"
      }
    }
  ]
}