{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1GXzzMDHst1sXErHSJ3zKiQ4PKDUYRAGx",
      "authorship_tag": "ABX9TyP0A2wWoT7W/NCzYzzhR11U",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/VytautasBruzila/Baigiamasis-darbas-JackDaniels/blob/main/Final_Version_of_Jack_Daniels_fake_bottle_detection.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WyadrhyG3Cp6",
        "outputId": "eb0dc47b-d2df-4e9d-8308-655fcce28c7f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import cv2\n",
        "import numpy as np\n",
        "from keras.applications.imagenet_utils import preprocess_input\n",
        "from keras.preprocessing.image import img_to_array\n",
        "\n",
        "# Define the paths to the original datasets\n",
        "fake_dir = '/content/drive/MyDrive/datasets/fake'\n",
        "original_dir = '/content/drive/MyDrive/datasets/original'\n",
        "\n",
        "# Define the paths to the preprocessed datasets\n",
        "preprocessed_fake_dir = '/content/drive/MyDrive/datasets/preprocessed/fake'\n",
        "preprocessed_original_dir = '/content/drive/MyDrive/datasets/preprocessed/original'\n",
        "\n",
        "# Create the preprocessed directories if they don't exist\n",
        "os.makedirs(preprocessed_fake_dir, exist_ok=True)\n",
        "os.makedirs(preprocessed_original_dir, exist_ok=True)\n",
        "\n",
        "# Define the target size for the images\n",
        "target_size = (512, 512)\n",
        "\n",
        "# Function to preprocess an image\n",
        "def preprocess_image(image_path, output_dir, label):\n",
        "    # Load the image\n",
        "    image = cv2.imread(image_path)\n",
        "\n",
        "    # Resize the image\n",
        "    resized_image = cv2.resize(image, target_size)\n",
        "\n",
        "    # Convert to grayscale\n",
        "    grayscale_image = cv2.cvtColor(resized_image, cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "    # Normalize the image\n",
        "    normalized_image = preprocess_input(grayscale_image)\n",
        "\n",
        "    # Convert to NumPy array\n",
        "    numpy_image = img_to_array(normalized_image)\n",
        "\n",
        "    # Save the preprocessed image\n",
        "    output_path = os.path.join(output_dir, f\"{label}_{os.path.basename(image_path)}\")\n",
        "    cv2.imwrite(output_path, numpy_image)\n",
        "\n",
        "# Preprocess the fake images\n",
        "for filename in os.listdir(fake_dir):\n",
        "    image_path = os.path.join(fake_dir, filename)\n",
        "    preprocess_image(image_path, preprocessed_fake_dir, 'fake')\n",
        "\n",
        "# Preprocess the original images\n",
        "for filename in os.listdir(original_dir):\n",
        "    image_path = os.path.join(original_dir, filename)\n",
        "    preprocess_image(image_path, preprocessed_original_dir, 'original')"
      ],
      "metadata": {
        "id": "MB79TdEBDW_Q"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "import numpy as np\n",
        "\n",
        "# Initialize the CNN\n",
        "model = Sequential()\n",
        "\n",
        "# Add the convolutional layer\n",
        "model.add(Conv2D(32, (3, 3), input_shape=(512, 512, 3), activation='relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "\n",
        "# Add another convolutional layer\n",
        "model.add(Conv2D(64, (3, 3), activation='relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "\n",
        "# Add another convolutional layer\n",
        "model.add(Conv2D(128, (3, 3), activation='relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "\n",
        "# Flattening\n",
        "model.add(Flatten())\n",
        "\n",
        "# Full connection\n",
        "model.add(Dense(units=128, activation='relu'))\n",
        "model.add(Dense(units=1, activation='sigmoid'))\n",
        "\n",
        "# Compile the CNN\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Part 2 - Fitting the CNN to the images\n",
        "train_datagen = ImageDataGenerator(rescale=1./255)\n",
        "test_datagen = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "training_set = train_datagen.flow_from_directory(\n",
        "    '/content/drive/MyDrive/datasets/preprocessed',\n",
        "    target_size=(512, 512),\n",
        "    batch_size=32,\n",
        "    class_mode='binary'\n",
        ")\n",
        "\n",
        "test_set = test_datagen.flow_from_directory(\n",
        "    '/content/drive/MyDrive/datasets/preprocessed',\n",
        "    target_size=(512, 512),\n",
        "    batch_size=32,\n",
        "    class_mode='binary'\n",
        ")\n",
        "\n",
        "model.fit_generator(\n",
        "    training_set,\n",
        "    steps_per_epoch=len(training_set) // 8,\n",
        "    epochs=25,\n",
        "    validation_data=test_set,\n",
        "    validation_steps=len(test_set) // 8\n",
        ")\n",
        "\n",
        "# Part 3 - Making new predictions\n",
        "def predict_image(file):\n",
        "    test_image = image.load_img(file, target_size=(512, 512))\n",
        "    test_image = image.img_to_array(test_image)\n",
        "    test_image = np.expand_dims(test_image, axis=0)\n",
        "    result = model.predict(test_image)\n",
        "    if result[0][0] >= 0.5:\n",
        "        prediction = 'fake'\n",
        "    else:\n",
        "        prediction = 'original'\n",
        "    return prediction"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UxT4lN0sTfhU",
        "outputId": "f5d3f87f-7265-490a-a861-0914dc40e95e"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 417 images belonging to 2 classes.\n",
            "Found 417 images belonging to 2 classes.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-22-54d1cd826019>:49: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
            "  model.fit_generator(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/25\n",
            "1/1 [==============================] - 34s 34s/step - loss: 0.7289 - accuracy: 0.3125 - val_loss: 25.6385 - val_accuracy: 0.6250\n",
            "Epoch 2/25\n",
            "1/1 [==============================] - 11s 11s/step - loss: 8.3109e-38 - accuracy: 1.0000 - val_loss: 44.2613 - val_accuracy: 0.6250\n",
            "Epoch 3/25\n",
            "1/1 [==============================] - 31s 31s/step - loss: 25.4691 - accuracy: 0.7812 - val_loss: 19.0746 - val_accuracy: 0.5938\n",
            "Epoch 4/25\n",
            "1/1 [==============================] - 39s 39s/step - loss: 22.5500 - accuracy: 0.5312 - val_loss: 3.9781 - val_accuracy: 0.2812\n",
            "Epoch 5/25\n",
            "1/1 [==============================] - 31s 31s/step - loss: 3.8092 - accuracy: 0.3125 - val_loss: 0.0952 - val_accuracy: 0.9688\n",
            "Epoch 6/25\n",
            "1/1 [==============================] - 31s 31s/step - loss: 0.0495 - accuracy: 0.9688 - val_loss: 0.5617 - val_accuracy: 0.6875\n",
            "Epoch 7/25\n",
            "1/1 [==============================] - 32s 32s/step - loss: 0.5570 - accuracy: 0.6562 - val_loss: 0.1225 - val_accuracy: 1.0000\n",
            "Epoch 8/25\n",
            "1/1 [==============================] - 9s 9s/step - loss: 0.5858 - accuracy: 1.0000 - val_loss: 0.0215 - val_accuracy: 1.0000\n",
            "Epoch 9/25\n",
            "1/1 [==============================] - 30s 30s/step - loss: 0.0178 - accuracy: 1.0000 - val_loss: 0.0925 - val_accuracy: 1.0000\n",
            "Epoch 10/25\n",
            "1/1 [==============================] - 32s 32s/step - loss: 0.1415 - accuracy: 0.9375 - val_loss: 0.0162 - val_accuracy: 1.0000\n",
            "Epoch 11/25\n",
            "1/1 [==============================] - 29s 29s/step - loss: 0.0164 - accuracy: 1.0000 - val_loss: 0.0030 - val_accuracy: 1.0000\n",
            "Epoch 12/25\n",
            "1/1 [==============================] - 31s 31s/step - loss: 0.0039 - accuracy: 1.0000 - val_loss: 0.0254 - val_accuracy: 1.0000\n",
            "Epoch 13/25\n",
            "1/1 [==============================] - 31s 31s/step - loss: 0.0212 - accuracy: 1.0000 - val_loss: 0.0556 - val_accuracy: 1.0000\n",
            "Epoch 14/25\n",
            "1/1 [==============================] - 30s 30s/step - loss: 0.0600 - accuracy: 1.0000 - val_loss: 0.0012 - val_accuracy: 1.0000\n",
            "Epoch 15/25\n",
            "1/1 [==============================] - 31s 31s/step - loss: 0.0013 - accuracy: 1.0000 - val_loss: 2.0652e-04 - val_accuracy: 1.0000\n",
            "Epoch 16/25\n",
            "1/1 [==============================] - 32s 32s/step - loss: 3.1124e-04 - accuracy: 1.0000 - val_loss: 2.1323e-04 - val_accuracy: 1.0000\n",
            "Epoch 17/25\n",
            "1/1 [==============================] - 31s 31s/step - loss: 1.2985e-04 - accuracy: 1.0000 - val_loss: 1.0102e-04 - val_accuracy: 1.0000\n",
            "Epoch 18/25\n",
            "1/1 [==============================] - 31s 31s/step - loss: 1.2207e-04 - accuracy: 1.0000 - val_loss: 8.9262e-05 - val_accuracy: 1.0000\n",
            "Epoch 19/25\n",
            "1/1 [==============================] - 35s 35s/step - loss: 5.7069e-05 - accuracy: 1.0000 - val_loss: 4.5619e-05 - val_accuracy: 1.0000\n",
            "Epoch 20/25\n",
            "1/1 [==============================] - 29s 29s/step - loss: 8.0420e-05 - accuracy: 1.0000 - val_loss: 1.5578e-04 - val_accuracy: 1.0000\n",
            "Epoch 21/25\n",
            "1/1 [==============================] - 31s 31s/step - loss: 3.1709e-05 - accuracy: 1.0000 - val_loss: 0.0061 - val_accuracy: 1.0000\n",
            "Epoch 22/25\n",
            "1/1 [==============================] - 41s 41s/step - loss: 1.0488e-05 - accuracy: 1.0000 - val_loss: 2.3102e-05 - val_accuracy: 1.0000\n",
            "Epoch 23/25\n",
            "1/1 [==============================] - 29s 29s/step - loss: 2.1606e-05 - accuracy: 1.0000 - val_loss: 6.6116e-06 - val_accuracy: 1.0000\n",
            "Epoch 24/25\n",
            "1/1 [==============================] - 29s 29s/step - loss: 1.3901e-05 - accuracy: 1.0000 - val_loss: 4.9657e-06 - val_accuracy: 1.0000\n",
            "Epoch 25/25\n",
            "1/1 [==============================] - 31s 31s/step - loss: 8.6926e-06 - accuracy: 1.0000 - val_loss: 2.5805e-06 - val_accuracy: 1.0000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def upload_file_for_prediction():\n",
        "    file_path = input(\"Enter the file path for prediction: \")\n",
        "    if file_path:\n",
        "        # Import necessary modules within the function\n",
        "        from keras.preprocessing import image\n",
        "        import numpy as np\n",
        "\n",
        "        # Predict image class\n",
        "        test_image = image.load_img(file_path, target_size=(512, 512))\n",
        "        test_image = image.img_to_array(test_image)\n",
        "        test_image = np.expand_dims(test_image, axis=0)\n",
        "        result = model.predict(test_image)\n",
        "        if result[0][0] >= 0.1:\n",
        "            prediction = 'Original'\n",
        "        else:\n",
        "            prediction = 'Fake'\n",
        "\n",
        "        print('File \"{}\" is predicted as: {}'.format(file_path, prediction))\n",
        "    else:\n",
        "        print(\"No file path entered.\")\n",
        "\n",
        "# Call the function to upload file for prediction\n",
        "upload_file_for_prediction()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WUXkf5OAamTp",
        "outputId": "53f6ccf8-f073-4993-8633-fb25a2235c47"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter the file path for prediction: /content/drive/MyDrive/datasets/fake/How to identify fake Jack Daniels_000060.jpg\n",
            "1/1 [==============================] - 0s 186ms/step\n",
            "File \"/content/drive/MyDrive/datasets/fake/How to identify fake Jack Daniels_000060.jpg\" is predicted as: Fake\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This tool make the checking procedure much more simplier."
      ],
      "metadata": {
        "id": "HxibEGhnrRNm"
      }
    }
  ]
}